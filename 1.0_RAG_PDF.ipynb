{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9096701",
   "metadata": {},
   "source": [
    "### Write below code to test if objects inside '.env' folder are loaded into this program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae62849",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ['GOOGLE_API_KEY']:\n",
    "    print(\"API key is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8546ddf",
   "metadata": {},
   "source": [
    "### Initial imports required from langchain framework for chunking, creating embeddings, reading pdf, calling chat model, creating vector db, doing similarity search and retreival are all declared or imported here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a05ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  #doeschunking\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings      #convertstextintovectors i.e.create embeddings\n",
    "from langchain.chat_models import init_chat_model     \n",
    "from langchain_community.document_loaders import PyPDFLoader        #to call the chat model gemini\n",
    "#from langchain_core.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=init_chat_model('google_genai:gemini-2.5-flash-lite')          #model name we use is defined here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dea9ce",
   "metadata": {},
   "source": [
    "### This is how you get your ll model to answer your questions by using method invoke and print it using variablename.content. This is only to test if llm model is working. It is not part of this rag project. only FYI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model.invoke(\"What is AI?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99859073",
   "metadata": {},
   "source": [
    "### Now lets load our PDF by giving the relative path of pdf and then using loader function to load the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/vidyashreerayar/Downloads/Learning and courses/My Projects/RAG/Ansh_Lamba_Tutorial/Festo_File_Overview.pdf\"  #Sets the file path of the PDF to be loaded\n",
    "loader = PyPDFLoader(pdf_path)          #Creates a PDF loader object called 'loader' to read the file\n",
    "docs = loader.load()                    #Loads and extracts the text content from the PDF vie loader object\n",
    "#docs                                  #print the loaded pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d3992",
   "metadata": {},
   "source": [
    "### The PDF is now loaded and our next step is to create chunks of this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) #Configure text splitter: 30-char chunks with 10-char overlap to preserve context\n",
    "chunks = splitter.split_documents(docs)                                    #Split the loaded documents into overlapping text chunks\n",
    "#len(chunks)                                                                #Get total number of generated chunks\n",
    "#chunks[0:20]                                                               #Preview the first 20 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b7b6b",
   "metadata": {},
   "source": [
    "### Now we covert our text into embeddings/vectors using embedding model of google gen ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f39517",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80e087",
   "metadata": {},
   "source": [
    "### For learning purpose only, below is how you can convert any text into embeddings i.e vectors using the method embed_query for smaller text and embed_documents for large amount of text or multiple docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095e9e7",
   "metadata": {},
   "source": [
    "#### embed_documents is used to generate embeddings for multiple texts (such as document chunks) so they can be stored in a vector database.\n",
    "\n",
    "#### embed_query is used to generate an embedding for a single search query, optimized for similarity matching against the stored document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_model.embed_query(\"what\")\n",
    "print(len(embeddings))  #Convert the text What into embeddings or vectors\n",
    "#embeddings                                        #print the generated embeddings or vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277ae27",
   "metadata": {},
   "source": [
    "### As next step, we create a vector store from langchain called Chromadb or Chroma. Langchain does all the work for you, all you have to provide is the name of the chunks variable in which you have all your chunks created from earlier steps, as the first input variable  and the variable that holds your embeding model name as your second input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_chunks = chunks[:100]   # first 100 chunks only\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=small_chunks, \n",
    "    embedding = embedding_model, \n",
    "    persist_directory=\"/Users/vidyashreerayar/Downloads/Learning and courses/My Projects/RAG/Ansh_Lamba_Tutorial/Vectorstore\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4909b8",
   "metadata": {},
   "source": [
    "### We now do semantic search using similarity search method of vectorstore. In this step, along with the query you want to ask, you need to give the number of top matches your RAG should retrieve from your PDF to match the answer to your question, say k=n, so it returns top n matching results to answer your question, we can use it as a context to reference it in the next step when we talk to LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6eda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = vectorstore.similarity_search(\"what is main.cpp\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a37104",
   "metadata": {},
   "source": [
    "### Next step is to ** Talk to LLM **, i.e as a user of this RAG, you can instruct the LLM to refer to the context variable we created above which is nothing but referring to the PDF file for answer match.You can do it by passing the context variable in a f string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(f\"What is main.cpp, You can refer to the context:{context}\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093eca4f",
   "metadata": {},
   "source": [
    "### Whatever IF we restart the kernel, the vector store gets vanished because this is an inmemory vector store and gets deleted everytime you refresh memory, to enable history and give memory to your RAG BOT, we create a directory called persist, using persist function. After that , all you have to do is querying step, because the RAG coding and its working in stored in persist directory or say a db. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8d328",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
